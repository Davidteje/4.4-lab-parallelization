{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Parallelization-Lab\" data-toc-modified-id=\"Parallelization-Lab-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Parallelization Lab</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-1:-Use-the-requests-library-to-retrieve-the-content-from-the-URL-below.\" data-toc-modified-id=\"Step-1:-Use-the-requests-library-to-retrieve-the-content-from-the-URL-below.-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Step 1: Use the requests library to retrieve the content from the URL below.</a></span></li><li><span><a href=\"#Step-2:-Use-BeautifulSoup-to-extract-a-list-of-all-the-unique-links-on-the-page.\" data-toc-modified-id=\"Step-2:-Use-BeautifulSoup-to-extract-a-list-of-all-the-unique-links-on-the-page.-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Step 2: Use BeautifulSoup to extract a list of all the unique links on the page.</a></span></li><li><span><a href=\"#Step-3:-Use-list-comprehensions-with-conditions-to-clean-the-link-list.\" data-toc-modified-id=\"Step-3:-Use-list-comprehensions-with-conditions-to-clean-the-link-list.-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Step 3: Use list comprehensions with conditions to clean the link list.</a></span></li><li><span><a href=\"#Step-4:-Use-the-os-library-to-create-a-folder-called-wikipedia-and-make-that-the-current-working-directory.\" data-toc-modified-id=\"Step-4:-Use-the-os-library-to-create-a-folder-called-wikipedia-and-make-that-the-current-working-directory.-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Step 4: Use the os library to create a folder called <em>wikipedia</em> and make that the current working directory.</a></span></li><li><span><a href=\"#Step-5:-Write-a-function-called-index_page-that-accepts-a-link-and-does-the-following.\" data-toc-modified-id=\"Step-5:-Write-a-function-called-index_page-that-accepts-a-link-and-does-the-following.-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Step 5: Write a function called index_page that accepts a link and does the following.</a></span></li><li><span><a href=\"#Step-6:-Sequentially-loop-through-the-list-of-links,-running-the-index_page-function-each-time.\" data-toc-modified-id=\"Step-6:-Sequentially-loop-through-the-list-of-links,-running-the-index_page-function-each-time.-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Step 6: Sequentially loop through the list of links, running the index_page function each time.</a></span></li><li><span><a href=\"#Step-7:-Perform-the-page-indexing-in-parallel-and-note-the-difference-in-performance.\" data-toc-modified-id=\"Step-7:-Perform-the-page-indexing-in-parallel-and-note-the-difference-in-performance.-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Step 7: Perform the page indexing in parallel and note the difference in performance.</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelization Lab\n",
    "\n",
    "In this lab, you will be leveraging several concepts you have learned to obtain a list of links from a web page and crawl and index the pages referenced by those links - both sequentially and in parallel. Follow the steps below to complete the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Use the requests library to retrieve the content from the URL below.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Data_science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Data_science'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#html=req.get(url).text\n",
    "#html[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Use BeautifulSoup to extract a list of all the unique links on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup=bs(html, 'html.parser')\n",
    "\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup1 = soup.find('div', id='content')\n",
    "#soup1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/wiki/File:PIA23792-1600x1200(1).jpg'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup1.find_all('a')[5].attrs['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2 = soup1.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#mw-head',\n",
       " '#searchInput',\n",
       " '/wiki/Information_science',\n",
       " '/wiki/File:PIA23792-1600x1200(1).jpg',\n",
       " '/wiki/File:PIA23792-1600x1200(1).jpg',\n",
       " '/wiki/Comet_NEOWISE',\n",
       " '/wiki/Astronomical_survey',\n",
       " '/wiki/Space_telescope',\n",
       " '/wiki/Wide-field_Infrared_Survey_Explorer',\n",
       " '/wiki/Interdisciplinary']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = []\n",
    "\n",
    "for i in range(len(soup2)):\n",
    "    try:\n",
    "        link = soup2[i].attrs['href']\n",
    "        lst.append(link)\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "lst[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#mw-head',\n",
       " '#searchInput',\n",
       " '/wiki/Information_science',\n",
       " '/wiki/File:PIA23792-1600x1200(1).jpg',\n",
       " '/wiki/File:PIA23792-1600x1200(1).jpg',\n",
       " '/wiki/Comet_NEOWISE',\n",
       " '/wiki/Astronomical_survey',\n",
       " '/wiki/Space_telescope',\n",
       " '/wiki/Wide-field_Infrared_Survey_Explorer',\n",
       " '/wiki/Interdisciplinary',\n",
       " '/wiki/Scientific_method',\n",
       " '/wiki/Algorithm',\n",
       " '/wiki/Knowledge',\n",
       " '/wiki/Unstructured_data',\n",
       " '#cite_note-1',\n",
       " '#cite_note-2',\n",
       " '/wiki/Data_mining',\n",
       " '/wiki/Machine_learning',\n",
       " '/wiki/Big_data',\n",
       " '/wiki/Computational_statistics',\n",
       " '/wiki/Analytics',\n",
       " '#cite_note-:2-3',\n",
       " '/wiki/Statistics',\n",
       " '/wiki/Data_analysis',\n",
       " '/wiki/Informatics',\n",
       " '/wiki/Scientific_method',\n",
       " '/wiki/Phenomena',\n",
       " '/wiki/Data',\n",
       " '#cite_note-4',\n",
       " '/wiki/Mathematics',\n",
       " '/wiki/Computer_science',\n",
       " '/wiki/Information_science',\n",
       " '/wiki/Domain_knowledge',\n",
       " '#cite_note-:2-3',\n",
       " '/wiki/Computer_science',\n",
       " '/wiki/Turing_Award',\n",
       " '/wiki/Jim_Gray_(computer_scientist)',\n",
       " '/wiki/Empirical_research',\n",
       " '/wiki/Basic_research',\n",
       " '/wiki/Computational_science',\n",
       " '/wiki/Information_technology',\n",
       " '/wiki/Information_explosion',\n",
       " '#cite_note-TansleyTolle2009-5',\n",
       " '#cite_note-BellHey2009-6',\n",
       " '#cite_note-7',\n",
       " '#Foundations',\n",
       " '#Relationship_to_statistics',\n",
       " '#Etymology',\n",
       " '#Early_usage',\n",
       " '#Modern_usage',\n",
       " '#See_also',\n",
       " '#References',\n",
       " '/w/index.php?title=Data_science&action=edit&section=1',\n",
       " '/wiki/Interdisciplinarity',\n",
       " '/wiki/Academic_discipline',\n",
       " '#cite_note-8',\n",
       " '/wiki/Big_data',\n",
       " '/wiki/Data_set',\n",
       " '/wiki/Problem_solving',\n",
       " '#cite_note-9',\n",
       " '/wiki/Analysis',\n",
       " '/wiki/Data_visualization',\n",
       " '/wiki/Information_visualization',\n",
       " '/wiki/Data_sonification',\n",
       " '/wiki/Data_integration',\n",
       " '/wiki/Graphic_design',\n",
       " '/wiki/Complex_systems',\n",
       " '/wiki/Communication',\n",
       " '/wiki/Business',\n",
       " '#cite_note-10',\n",
       " '#cite_note-11',\n",
       " '/wiki/Nathan_Yau',\n",
       " '/wiki/Ben_Fry',\n",
       " '/wiki/Human%E2%80%93computer_interaction',\n",
       " '/wiki/Exploration',\n",
       " '#cite_note-12',\n",
       " '#cite_note-13',\n",
       " '/wiki/American_Statistical_Association',\n",
       " '/wiki/Database',\n",
       " '/wiki/Machine_learning',\n",
       " '/wiki/Distributed_computing',\n",
       " '#cite_note-14',\n",
       " '/w/index.php?title=Data_science&action=edit&section=2',\n",
       " '/wiki/Nate_Silver',\n",
       " '#cite_note-15',\n",
       " '#cite_note-16',\n",
       " '/wiki/Vasant_Dhar',\n",
       " '#cite_note-17',\n",
       " '/wiki/Andrew_Gelman',\n",
       " '/wiki/Columbia_University',\n",
       " '#cite_note-18',\n",
       " '/wiki/David_Donoho',\n",
       " '#cite_note-:7-19',\n",
       " '/w/index.php?title=Data_science&action=edit&section=3',\n",
       " '/w/index.php?title=Data_science&action=edit&section=4',\n",
       " '/wiki/John_Tukey',\n",
       " '#cite_note-:7-19',\n",
       " '/wiki/C._F._Jeff_Wu',\n",
       " '#cite_note-20',\n",
       " '/wiki/Montpellier_2_University']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Use list comprehensions with conditions to clean the link list.\n",
    "\n",
    "There are two types of links, absolute and relative. Absolute links have the full URL and begin with http while relative links begin with a forward slash (/) and point to an internal page within the wikipedia.org domain. Clean the respective types of URLs as follows.\n",
    "\n",
    "- Absolute Links: Create a list of these and remove any that contain a percentage sign (%).\n",
    "- Relativel Links: Create a list of these, add the domain to the link so that you have the full URL, and remove any that contain a percentage sign (%).\n",
    "- Combine the list of absolute and relative links and ensure there are no duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'http://wikipedia.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_link = []\n",
    "rel_link = []\n",
    "\n",
    "for i in lst:\n",
    "    if i[0] == 'h' and '%' not in i:\n",
    "        abs_link.append(i)\n",
    "    elif i[0] == '/' and '%' not in i:\n",
    "        rel_link.append(domain + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://cacm.acm.org/magazines/2013/12/169933-data-science-and-prediction/fulltext',\n",
       " 'https://api.semanticscholar.org/CorpusID:6107147',\n",
       " 'https://web.archive.org/web/20141109113411/http://cacm.acm.org/magazines/2013/12/169933-data-science-and-prediction/fulltext',\n",
       " 'http://simplystatistics.org/2013/12/12/the-key-word-in-data-science-is-not-data-it-is-science/',\n",
       " 'https://web.archive.org/web/20140102194117/http://simplystatistics.org/2013/12/12/the-key-word-in-data-science-is-not-data-it-is-science/',\n",
       " 'https://api.semanticscholar.org/CorpusID:207595944',\n",
       " 'https://www.springer.com/book/9784431702085',\n",
       " 'https://books.google.com/books?id=oGs_AQAAIAAJ',\n",
       " 'https://web.archive.org/web/20170320193019/https://books.google.com/books?id=oGs_AQAAIAAJ',\n",
       " 'https://api.semanticscholar.org/CorpusID:9743327']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_link[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abs_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://wikipedia.org/wiki/Information_science',\n",
       " 'http://wikipedia.org/wiki/File:PIA23792-1600x1200(1).jpg',\n",
       " 'http://wikipedia.org/wiki/File:PIA23792-1600x1200(1).jpg',\n",
       " 'http://wikipedia.org/wiki/Comet_NEOWISE',\n",
       " 'http://wikipedia.org/wiki/Astronomical_survey',\n",
       " 'http://wikipedia.org/wiki/Space_telescope',\n",
       " 'http://wikipedia.org/wiki/Wide-field_Infrared_Survey_Explorer',\n",
       " 'http://wikipedia.org/wiki/Interdisciplinary',\n",
       " 'http://wikipedia.org/wiki/Scientific_method',\n",
       " 'http://wikipedia.org/wiki/Algorithm']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_link[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rel_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_link = rel_link + abs_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tot_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://wikipedia.org/wiki/Phenomena',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Template:Data&action=edit',\n",
       " 'http://wikipedia.org/wiki/Data_library',\n",
       " 'http://wikipedia.org/wiki/Nate_Silver',\n",
       " 'http://wikipedia.org/wiki/Committee_on_Data_for_Science_and_Technology',\n",
       " 'https://benfry.com/phd/dissertation/2.html',\n",
       " 'http://wikipedia.org/wiki/Data_set',\n",
       " 'https://api.semanticscholar.org/CorpusID:9743327',\n",
       " 'http://wikipedia.org/w/index.php?title=Data_science&action=edit&section=2',\n",
       " 'http://wikipedia.org/wiki/New_York_City']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_link_f = list(set(tot_link))\n",
    "tot_link_f[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tot_link_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Use the os library to create a folder called *wikipedia* and make that the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\david\\\\Lab\\\\4.4-lab-parallelization\\\\your-code'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.mkdir(\"wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\david\\\\Lab\\\\4.4-lab-parallelization\\\\your-code\\\\wikipedia'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Write a function called index_page that accepts a link and does the following.\n",
    "\n",
    "- Tries to request the content of the page referenced by that link.\n",
    "- Slugifies the filename using the `slugify` function from the [python-slugify](https://pypi.org/project/python-slugify/) library and adds a .html file extension.\n",
    "    - If you don't already have the python-slugify library installed, you can pip install it as follows: `$ pip install python-slugify`.\n",
    "    - To import the slugify function, you would do the following: `from slugify import slugify`.\n",
    "    - You can then slugify a link as follows `slugify(link)`.\n",
    "- Creates a file in the wikipedia folder using the slugified filename and writes the contents of the page to the file.\n",
    "- If an exception occurs during the process above, just `pass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-slugify in c:\\users\\david\\anaconda3\\envs\\clase\\lib\\site-packages (6.1.2)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\david\\anaconda3\\envs\\clase\\lib\\site-packages (from python-slugify) (1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-slugify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slugify import slugify\n",
    "\n",
    "unicode=str()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'http://wikipedia.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http-wikipedia-org'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slugify(domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE html>\\n<html lang=\"en\" class=\"no-js\">\\n<head>\\n<meta charset=\"utf-8\">\\n<title>Wikipedia</title'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html2 = req.get(domain).content\n",
    "html2[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_page(s):\n",
    "    \n",
    "    html = req.get(s).content\n",
    "    texto = bs(html, 'html.parser').text\n",
    "\n",
    "    archivo = slugify(s) + '.txt'\n",
    "\n",
    "    f = open(archivo, 'w' , encoding=\"utf-8\")\n",
    "    f.write(slugify(str(texto)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_page(domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Sequentially loop through the list of links, running the index_page function each time.\n",
    "\n",
    "Remember to include `%%time` at the beginning of the cell so that it measures the time it takes for the cell to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in tot_link_f:\n",
    "    try:\n",
    "        index_page(i)\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Perform the page indexing in parallel and note the difference in performance.\n",
    "\n",
    "Remember to include `%%time` at the beginning of the cell so that it measures the time it takes for the cell to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from multiprocessing import get_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting joblib\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "     -------------------------------------- 298.0/298.0 kB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: joblib\n",
      "Successfully installed joblib-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   13.7s\n"
     ]
    },
    {
     "ename": "SSLError",
     "evalue": "None: Max retries exceeded with url: /handle/2451/31553 (Caused by None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\david\\anaconda3\\envs\\clase\\lib\\site-packages\\urllib3\\connectionpool.py\", line 710, in urlopen\n    chunked=chunked,\n  File \"C:\\Users\\david\\anaconda3\\envs\\clase\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"C:\\Users\\david\\anaconda3\\envs\\clase\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n    conn.connect()\n  File \"C:\\Users\\david\\anaconda3\\envs\\clase\\lib\\site-packages\\urllib3\\connection.py\", line 424, in connect\n    tls_in_tls=tls_in_tls,\n  File \"C:\\Users\\david\\anaconda3\\envs\\clase\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 450, in ssl_wrap_socket\n    sock, context, tls_in_tls, server_hostname=server_hostname\n  File \"C:\\Users\\david\\anaconda3\\envs\\clase\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"C:\\Users\\david\\anaconda3\\envs\\clase\\lib\\ssl.py\", line 423, in wrap_socket\n    session=session\n  File \"C:\\Users\\david\\anaconda3\\envs\\clase\\lib\\ssl.py\", line 870, in _create\n    self.do_handshake()\n  File \"C:\\Users\\david\\anaconda3\\envs\\clase\\lib\\ssl.py\", line 1139, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1091)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\david\\anaconda3\\envs\\clase\\lib\\site-packages\\requests\\adapters.py\", line 499, in send\n    timeout=timeout,\n  File \"C:\\Users\\david\\anaconda3\\envs\\clase\\lib\\site-packages\\urllib3\\connectionpool.py\", line 788, in urlopen\n    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n  File \"C:\\Users\\david\\anaconda3\\envs\\clase\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='archive.nyu.edu', port=443): Max retries exceeded with url: /handle/2451/31553 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1091)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\david\\anaconda3\\envs\\clase\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 428, in _process_worker\n    r = call_item()\n  File \"C:\\Users\\david\\anaconda3\\envs\\clase\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\Users\\david\\anaconda3\\envs\\clase\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\david\\anaconda3\\envs\\clase\\lib\\site-packages\\joblib\\parallel.py\", line 289, in __call__\n    for func, args, kwargs in self.items]\n  File \"C:\\Users\\david\\anaconda3\\envs\\clase\\lib\\site-packages\\joblib\\parallel.py\", line 289, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_22216\\1064165821.py\", line 3, in index_page\n  File \"C:\\Users\\david\\anaconda3\\envs\\clase\\lib\\site-packages\\requests\\api.py\", line 73, in get\n    return request(\"get\", url, params=params, **kwargs)\n  File \"C:\\Users\\david\\anaconda3\\envs\\clase\\lib\\site-packages\\requests\\api.py\", line 59, in request\n    return session.request(method=method, url=url, **kwargs)\n  File \"C:\\Users\\david\\anaconda3\\envs\\clase\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"C:\\Users\\david\\anaconda3\\envs\\clase\\lib\\site-packages\\requests\\sessions.py\", line 723, in send\n    history = [resp for resp in gen]\n  File \"C:\\Users\\david\\anaconda3\\envs\\clase\\lib\\site-packages\\requests\\sessions.py\", line 723, in <listcomp>\n    history = [resp for resp in gen]\n  File \"C:\\Users\\david\\anaconda3\\envs\\clase\\lib\\site-packages\\requests\\sessions.py\", line 274, in resolve_redirects\n    **adapter_kwargs,\n  File \"C:\\Users\\david\\anaconda3\\envs\\clase\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n    r = adapter.send(request, **kwargs)\n  File \"C:\\Users\\david\\anaconda3\\envs\\clase\\lib\\site-packages\\requests\\adapters.py\", line 563, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='archive.nyu.edu', port=443): Max retries exceeded with url: /handle/2451/31553 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1091)')))\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\clase\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\clase\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 975\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    976\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\clase\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    565\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    566\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 567\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    568\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\clase\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    426\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\clase\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSSLError\u001b[0m: None: Max retries exceeded with url: /handle/2451/31553 (Caused by None)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lst = Parallel(n_jobs=4, verbose=True)(delayed(index_page)(i) for i in tot_link_f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#\n",
    "#pool=mp.Pool(mp.cpu_count())    # usa todos los nucleos\n",
    "#\n",
    "#try:\n",
    "#    res=pool.imap(index_page, tot_link_f).get()\n",
    "#except:\n",
    "#    pass\n",
    "#\n",
    "#pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clase",
   "language": "python",
   "name": "clase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
